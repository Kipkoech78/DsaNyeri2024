{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1ysEKrw_LE2jMndo1snrZUh5w87LQsCxk","timestamp":1717170636860}],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["#Natural Language Processing\n","Natural Language Processing (or NLP for short) is a discipline in computing that deals with the communication between natural (human) languages and computer languages. A common example of NLP is something like spellcheck or autocomplete. Essentially NLP is the field that focuses on how computers can understand and/or process natural/human languages."],"metadata":{"id":"Xdq70c6L_HLe"}},{"cell_type":"code","source":["import re\n","import nltk\n","from nltk.corpus import stopwords\n","#nltk.download('stopwords') # Run this once\n","nltk.download('wordnet')\n","from nltk.stem import WordNetLemmatizer\n","stop_words = stopwords.words('english')\n","lemmatizer = WordNetLemmatizer()\n","\n","import spacy\n","\n","# Load the spaCy English model\n","nlp = spacy.load('en_core_web_sm')\n","\n","# Define a sample text\n","text = \"The quick brown foxes are jumping over the lazy dogs.\"\n","\n","\n","def clean_text(text):\n","\n","    text = text.lower()\n","\n","\n","    text = re.sub(r\"http\\S+\", \"\",text) #Removing URLs\n","    #text = re.sub(r\"http\", \"\",text)\n","\n","    html=re.compile(r'<.*?>')\n","    text = html.sub(r'',text) #Removing html tags\n","    punctuations = '@#!?+&*[]-%.:/();$=><|{}^' + \"'`\" + '_'\n","    for p in punctuations:\n","        text = text.replace(p,'') #Removing punctuations\n","    # Process the text using spaCy\n","    doc = nlp(text)\n","    POS_sent = [token.pos_ for token in doc]\n","    # Extract lemmatized tokens\n","    lemmatized_tokens = [token.lemma_ for token in doc]\n","\n","    print([(token,pos) for token,pos in zip(lemmatized_tokens,POS_sent)])\n","    text = [word.lower() for word in lemmatized_tokens if word.lower() not in stop_words] #removing stopwords\n","\n","     # Join the lemmatized tokens into a sentence\n","    text = ' '.join(text)\n","    \"\"\"emoji_pattern = re.compile(\"[\"\n","                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","                           u\"\\U00002702-\\U000027B0\"\n","                           u\"\\U000024C2-\\U0001F251\"\n","                           \"]+\", flags=re.UNICODE)\n","    text = emoji_pattern.sub(r'', text) #Removing emojis\"\"\"\n","\n","    return text"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cAgqcPRdZ2a5","executionInfo":{"status":"ok","timestamp":1717349099919,"user_tz":-180,"elapsed":1459,"user":{"displayName":"LILIAN D. WANZARE","userId":"15710368761207199104"}},"outputId":"beca61b5-70d4-4f1c-a84b-232d07312a2a"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}]},{"cell_type":"code","source":["#Text Normalization\n","CORPUS = [\n","'<br>the sky is blue.',\n","'sky is blue and sky is beautiful',\n","'the beautiful sky is so blue',\n","'i loved blue cheese!',\n","'the sky is beautifully made'\n","]\n","\n","text = [clean_text(t) for t in CORPUS]\n","print(text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qB6bOpye5oOF","executionInfo":{"status":"ok","timestamp":1717349102537,"user_tz":-180,"elapsed":427,"user":{"displayName":"LILIAN D. WANZARE","userId":"15710368761207199104"}},"outputId":"637d61a9-1dc3-486f-a093-b9fd3940325b"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["[('the', 'DET'), ('sky', 'NOUN'), ('be', 'AUX'), ('blue', 'ADJ')]\n","[('sky', 'NOUN'), ('be', 'AUX'), ('blue', 'ADJ'), ('and', 'CCONJ'), ('sky', 'NOUN'), ('be', 'AUX'), ('beautiful', 'ADJ')]\n","[('the', 'DET'), ('beautiful', 'ADJ'), ('sky', 'NOUN'), ('be', 'AUX'), ('so', 'ADV'), ('blue', 'ADJ')]\n","[('I', 'PRON'), ('love', 'VERB'), ('blue', 'ADJ'), ('cheese', 'NOUN')]\n","[('the', 'DET'), ('sky', 'NOUN'), ('be', 'AUX'), ('beautifully', 'ADV'), ('make', 'VERB')]\n","['sky blue', 'sky blue sky beautiful', 'beautiful sky blue', 'love blue cheese', 'sky beautifully make']\n"]}]},{"cell_type":"markdown","metadata":{"id":"ur_FQq-Q-fxC"},"source":["## Sequence Data\n","In the previous tutorials we focused on data that we could represent as one static data point where the notion of time or step was irrelevant. Take for example our image data, it was simply a tensor of shape (width, height, channels). That data doesn't change or care about the notion of time.\n","\n","In this tutorial we will look at sequences of text and learn how we can encode them in a meaningful way. Unlike images, sequence data such as long chains of text, weather patterns, videos and really anything where the notion of a step or time is relevant needs to be processed and handled in a special way.\n","\n","But what do I mean by sequences and why is text data a sequence? Well that's a good question. Since textual data contains many words that follow in a very specific and meaningful order, we need to be able to keep track of each word and when it occurs in the data. Simply encoding say an entire paragraph of text into one data point wouldn't give us a very meaningful picture of the data and would be very difficult to do anything with. This is why we treat text as a sequence and process one word at a time. We will keep track of where each of these words appear and use that information to try to understand the meaning of peices of text.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"8gQHK4V4e2wl"},"source":["##Encoding Text\n","As we know machine learning models and neural networks don't take raw text data as an input. This means we must somehow encode our textual data to numeric values that our models can understand. There are many different ways of doing this and we will look at a few examples below.\n","\n","Before we get into the different encoding/preprocessing methods let's understand the information we can get from textual data by looking at the following two movie reviews.\n","\n","```I thought the movie was going to be bad, but it was actually amazing!```\n","\n","```I thought the movie was going to be amazing, but it was actually bad!```\n","\n","Although these two setences are very similar we know that they have very different meanings. This is because of the **ordering** of words, a very important property of textual data.\n","\n","Now keep that in mind while we consider some different ways of encoding our textual data.\n","\n","###Bag of Words\n","The first and simplest way to encode our data is to use something called **bag of words**. This is a pretty easy technique where each word in a sentence is encoded with an integer and thrown into a collection that does not maintain the order of the words but does keep track of the frequency. Have a look at the python function below that encodes a string of text into bag of words."]},{"cell_type":"code","metadata":{"id":"5KiCCBsIkMHi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1717340560288,"user_tz":-180,"elapsed":577,"user":{"displayName":"LILIAN D. WANZARE","userId":"15710368761207199104"}},"outputId":"b6f9c30c-4f4c-445f-e23d-a733d215a73a"},"source":["vocab = {}  # maps word to integer representing it\n","word_encoding = 1\n","def bag_of_words(text):\n","  global word_encoding\n","\n","  words = text.lower().split(\" \")  # create a list of all of the words in the text, well assume there is no grammar in our text for this example\n","  bag = {}  # stores all of the encodings and their frequency\n","\n","  for word in words:\n","    if word in vocab:\n","      encoding = vocab[word]  # get encoding from vocab\n","    else:\n","      vocab[word] = word_encoding\n","      encoding = word_encoding\n","      word_encoding += 1\n","\n","    if encoding in bag:\n","      bag[encoding] += 1\n","    else:\n","      bag[encoding] = 1\n","\n","  return bag\n","\n","text = \"this is a test to see if this test will work is is test a a\"\n","bag = bag_of_words(text)\n","print(bag)\n","print(vocab)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{1: 2, 2: 3, 3: 3, 4: 3, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1}\n","{'this': 1, 'is': 2, 'a': 3, 'test': 4, 'to': 5, 'see': 6, 'if': 7, 'will': 8, 'work': 9}\n"]}]},{"cell_type":"markdown","metadata":{"id":"4hEvstSBl1gy"},"source":["This isn't really the way we would do this in practice, but I hope it gives you an idea of how bag of words works. Notice that we've lost the order in which words appear. In fact, let's look at how this encoding works for the two sentences we showed above.\n","\n"]},{"cell_type":"code","metadata":{"id":"miYshfvzmJ0H","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1717340570128,"user_tz":-180,"elapsed":577,"user":{"displayName":"LILIAN D. WANZARE","userId":"15710368761207199104"}},"outputId":"679913f7-aed2-4038-c946-f39cd4e7c6f2"},"source":["positive_review = \"I thought the movie was going to be bad but it was actually amazing\"\n","negative_review = \"I thought the movie was going to be amazing but it was actually bad\"\n","\n","pos_bag = bag_of_words(positive_review)\n","neg_bag = bag_of_words(negative_review)\n","\n","print(\"Positive:\", pos_bag)\n","print(\"Negative:\", neg_bag)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Positive: {10: 1, 11: 1, 12: 1, 13: 1, 14: 2, 15: 1, 5: 1, 16: 1, 17: 1, 18: 1, 19: 1, 20: 1, 21: 1}\n","Negative: {10: 1, 11: 1, 12: 1, 13: 1, 14: 2, 15: 1, 5: 1, 16: 1, 21: 1, 18: 1, 19: 1, 20: 1, 17: 1}\n"]}]},{"cell_type":"markdown","metadata":{"id":"Pl7Fw9s3mkfK"},"source":["We can see that even though these sentences have a very different meaning they are encoded exaclty the same way. Obviously, this isn't going to fly. Let's look at some other methods.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"DUKTycffmu1k"},"source":["###Integer Encoding\n","The next technique we will look at is called **integer encoding**. This involves representing each word or character in a sentence as a unique integer and maintaining the order of these words. This should hopefully fix the problem we saw before were we lost the order of words.\n"]},{"cell_type":"code","metadata":{"id":"MKY4y_tjnUEW","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1717340575209,"user_tz":-180,"elapsed":562,"user":{"displayName":"LILIAN D. WANZARE","userId":"15710368761207199104"}},"outputId":"ba491ad1-8e3c-4b2f-fe73-e70dd68f982d"},"source":["vocab = {}\n","word_encoding = 1\n","def one_hot_encoding(text):\n","  global word_encoding\n","\n","  words = text.lower().split(\" \")\n","  encoding = []\n","\n","  for word in words:\n","    if word in vocab:\n","      code = vocab[word]\n","      encoding.append(code)\n","    else:\n","      vocab[word] = word_encoding\n","      encoding.append(word_encoding)\n","      word_encoding += 1\n","\n","  return encoding\n","\n","text = \"this is a test to see if this test will work is is test a a\"\n","encoding = one_hot_encoding(text)\n","print(encoding)\n","print(vocab)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[1, 2, 3, 4, 5, 6, 7, 1, 4, 8, 9, 2, 2, 4, 3, 3]\n","{'this': 1, 'is': 2, 'a': 3, 'test': 4, 'to': 5, 'see': 6, 'if': 7, 'will': 8, 'work': 9}\n"]}]},{"cell_type":"markdown","metadata":{"id":"TOrLG9Bin0Zv"},"source":["And now let's have a look at one hot encoding on our movie reviews."]},{"cell_type":"code","metadata":{"id":"1S-GNjotn-Br","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1717340579390,"user_tz":-180,"elapsed":4,"user":{"displayName":"LILIAN D. WANZARE","userId":"15710368761207199104"}},"outputId":"270a549e-6bd4-4656-b232-d7b1bdad9bac"},"source":["positive_review = \"I thought the movie was going to be bad but it was actually amazing\"\n","negative_review = \"I thought the movie was going to be amazing but it was actually bad\"\n","\n","pos_encode = one_hot_encoding(positive_review)\n","neg_encode = one_hot_encoding(negative_review)\n","\n","print(\"Positive:\", pos_encode)\n","print(\"Negative:\", neg_encode)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Positive: [10, 11, 12, 13, 14, 15, 5, 16, 17, 18, 19, 14, 20, 21]\n","Negative: [10, 11, 12, 13, 14, 15, 5, 16, 21, 18, 19, 14, 20, 17]\n"]}]},{"cell_type":"markdown","metadata":{"id":"jC9UYV4vpq6Y"},"source":["Much better, now we are keeping track of the order of words and we can tell where each occurs. But this still has a few issues with it. Ideally when we encode words, we would like similar words to have similar labels and different words to have very different labels. For example, the words happy and joyful should probably have very similar labels so we can determine that they are similar. While words like horrible and amazing should probably have very different labels. The method we looked at above won't be able to do something like this for us. This could mean that the model will have a very difficult time determing if two words are similar or not which could result in some pretty drastic performace impacts.\n","\n"]},{"cell_type":"markdown","source":["We can use SKlearn for BOW models"],"metadata":{"id":"M_mN6GVxDjOV"}},{"cell_type":"code","source":["CORPUS = [\n","'the sky is blue',\n","'sky is blue and sky is beautiful',\n","'the beautiful sky is so blue',\n","'i love blue cheese'\n","]\n","new_doc = ['loving this blue sky today']\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","vectorizer = CountVectorizer(min_df=1, ngram_range=(1,1))\n","features = vectorizer.fit_transform(CORPUS)\n","features_ = features.todense()\n","#That output shows how each text document has been converted to vectors\n","print(features_)\n","#Each column in a vector represents the words depicted in feature_names\n","feature_names = vectorizer.get_feature_names_out()\n","print(feature_names)\n","\n","# We can combine these two in a pandas dataframe to see\n","import pandas as pd\n","def display_features(features, feature_names):\n","  df = pd.DataFrame(data=features,columns=feature_names)\n","  print (df)\n","display_features(features_, feature_names)"],"metadata":{"id":"592gmXY9DkX6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1717340586537,"user_tz":-180,"elapsed":803,"user":{"displayName":"LILIAN D. WANZARE","userId":"15710368761207199104"}},"outputId":"0d22f356-d84e-4a09-865f-5183a37fb3e4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0 0 1 0 1 0 1 0 1]\n"," [1 1 1 0 2 0 2 0 0]\n"," [0 1 1 0 1 0 1 1 1]\n"," [0 0 1 1 0 1 0 0 0]]\n","['and' 'beautiful' 'blue' 'cheese' 'is' 'love' 'sky' 'so' 'the']\n","   and  beautiful  blue  cheese  is  love  sky  so  the\n","0    0          0     1       0   1     0    1   0    1\n","1    1          1     1       0   2     0    2   0    0\n","2    0          1     1       0   1     0    1   1    1\n","3    0          0     1       1   0     1    0   0    0\n"]}]},{"cell_type":"markdown","source":["# TF-IDF Model\n","The Bag of Words model is good, but the vectors are completely based on absolute frequencies of word occurrences. This has some potential problems where words that may tend to occur a lot across all documents in the corpus will have higher frequencies and will tend to overshadow other words that may not occur as frequently but may be more interesting and effective as features to identify specific categories for the documents. This is where TF-IDF comes into the picture. TF-IDF stands for Term Frequency-Inverse Document Frequency, a combination of two metrics: term frequency and inverse document frequency.\n","\n","Mathematically, TF-IDF is the product of two\n","metrics and can be represented as tfidftfidf =tf*idf , where term frequency (tf ) and inverse-document frequency (idf ) represent the two metrics.\n","\n","Term frequencydenoted by tf is what we had computed in the Bag of Words model.\n","\n","Inverse document frequencydenoted by idfis the inverse of the document frequency for each term. It is computed by dividing the total number of documents in our corpus by the document frequency for each term and then applying logarithmic scaling on the result.\n","\n","Mathematically our implementation for idfcan be represented by\n","\n","```math\n","`idf(t)=1+log C/1+df(t)\n","```"],"metadata":{"id":"7dt3OiloF7Sc"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfTransformer\n","import numpy as np\n","transformer = TfidfTransformer(smooth_idf=True,use_idf=True)\n","tdidf_features = transformer.fit_transform(features)\n","tdidf_features_ = np.round(tdidf_features.todense(), 2)\n","display_features(tdidf_features_, feature_names)\n","idf = transformer.idf_\n","print(\"\\nIDFs: \", idf)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9ckBCGghJa-L","executionInfo":{"status":"ok","timestamp":1717340591572,"user_tz":-180,"elapsed":544,"user":{"displayName":"LILIAN D. WANZARE","userId":"15710368761207199104"}},"outputId":"c5731cc2-45c9-41eb-9444-f1c3c2b95867"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["    and  beautiful  blue  cheese    is  love   sky    so   the\n","0  0.00       0.00  0.40    0.00  0.49  0.00  0.49  0.00  0.60\n","1  0.44       0.35  0.23    0.00  0.56  0.00  0.56  0.00  0.00\n","2  0.00       0.43  0.29    0.00  0.35  0.00  0.35  0.55  0.43\n","3  0.00       0.00  0.35    0.66  0.00  0.66  0.00  0.00  0.00\n","\n","IDFs:  [1.91629073 1.51082562 1.         1.91629073 1.22314355 1.91629073\n"," 1.22314355 1.91629073 1.51082562]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"7VNlb3D0irWW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Building a simple classifier\n","Download the IMDB dataset from Kaggle\n","\n","https://www.kaggle.com/code/neerajmohan/nlp-text-classification-using-tf-idf-features"],"metadata":{"id":"mIb7SOqcRTO_"}},{"cell_type":"code","source":["from google.colab import files\n","uploaded = files.upload()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":74},"id":"DN5RAF48it22","executionInfo":{"status":"ok","timestamp":1717344183075,"user_tz":-180,"elapsed":107961,"user":{"displayName":"LILIAN D. WANZARE","userId":"15710368761207199104"}},"outputId":"825b2a01-9cad-4bb7-b810-999ced241105"},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-2f1db077-f6d2-46c8-929c-d6ad42d4e9b7\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-2f1db077-f6d2-46c8-929c-d6ad42d4e9b7\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving IMDB_short.csv to IMDB_short.csv\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import io\n","\n","df = pd.read_csv(io.StringIO(uploaded['IMDB_short.csv'].decode('utf-8')))\n","df\n","df['review_sentiment']=df['sentiment'].apply(lambda x: 1 if x=='positive' else 0)\n","df.sample(5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"Xn7DusS8j0nl","executionInfo":{"status":"ok","timestamp":1717345714277,"user_tz":-180,"elapsed":1383,"user":{"displayName":"LILIAN D. WANZARE","userId":"15710368761207199104"}},"outputId":"dd576c8f-b1c3-42de-df6c-d9408e536fc9"},"execution_count":35,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                                                 review sentiment  \\\n","727   Fans of the HBO series \"Tales From the Crypt\" ...  positive   \n","3393  Released in December of 1957, Sayonara went on...  positive   \n","4607  If you're a long-time fan of the Doctor and yo...  positive   \n","4165  A young scientist is trying to carry on his de...  negative   \n","485   this film is quite simply one of the worst fil...  negative   \n","\n","      review_sentiment  \n","727                  1  \n","3393                 1  \n","4607                 1  \n","4165                 0  \n","485                  0  "],"text/html":["\n","  <div id=\"df-e8abe387-f9c2-4d7d-8305-d3002f234643\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>review</th>\n","      <th>sentiment</th>\n","      <th>review_sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>727</th>\n","      <td>Fans of the HBO series \"Tales From the Crypt\" ...</td>\n","      <td>positive</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3393</th>\n","      <td>Released in December of 1957, Sayonara went on...</td>\n","      <td>positive</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4607</th>\n","      <td>If you're a long-time fan of the Doctor and yo...</td>\n","      <td>positive</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4165</th>\n","      <td>A young scientist is trying to carry on his de...</td>\n","      <td>negative</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>485</th>\n","      <td>this film is quite simply one of the worst fil...</td>\n","      <td>negative</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e8abe387-f9c2-4d7d-8305-d3002f234643')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-e8abe387-f9c2-4d7d-8305-d3002f234643 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-e8abe387-f9c2-4d7d-8305-d3002f234643');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-24b05ab6-c1f7-4c47-99b5-c3e6d44f9c85\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-24b05ab6-c1f7-4c47-99b5-c3e6d44f9c85')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-24b05ab6-c1f7-4c47-99b5-c3e6d44f9c85 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe","summary":"{\n  \"name\": \"df\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"review\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Released in December of 1957, Sayonara went on to earn 8 Oscar nominations and would pull in 4 wins. Red Buttons won the Oscar for Best Supporting Actor in his role as airman Joe Kelly who falls in love with a Japanese woman while stationed in Kobe during the Korean War. Oscar nominated for Best Leading Actor, Marlon Brando plays Major Lloyd Gruver, a Korean War flying ace reassigned to Japan, who staunchly supports the military's opposition to marriages between American troops and Japanese women and tries without any success to talk his friend Joe Kelly out of getting married. Ironically Marlon Brandos character soon finds love of his own in a woman of Japanese descent. This movie highlights the prejudices and cultural differences of that time. Filmed in beautiful color and with stunning backgrounds I found this movie to be well worth watching just for these effects alone. Good movie, gimme more...GimmeClassics\",\n          \"this film is quite simply one of the worst films ever made and is a damning indictment on not only the British film industry but the talentless hacks at work today. Not only did the film get mainstream distribution it also features a good cast of British actors, so what went wrong? i don't know and simply i don't care enough to engage with the debate because the film was so terrible it deserves no thought at all. be warned and stay the hell away from this rubbish. but apparently i need to write ten lines of text in this review so i might as well detail the plot. A nob of a man is setup by his evil friend and co-worker out of his father's company and thus leads to an encounter with the Russian mafia and dodgy accents and stupid, very stupid plot twists/devices. i should have asked for my money back but was perhaps still in shock from the experience. if you want a good crime film watch the usual suspects or the godfather, what about lock, stock.... thats the peak of the contemporary British crime film.....\",\n          \"If you're a long-time fan of the Doctor and you cringed when you heard they were making another series, rest easy -- it more than meets the high expectations of the original. The pacing is much quicker than the original shows, fitting more often into 50 minutes episodes rather than the average 90 minutes. The writing is excellent, the acting superb. The hardest - and best - thing to get used to is the production values of the new series. Compared to the original, it's got some now. (Although I will always have fond memories of bubble-wrap and hand-puppet monsters.) If you're not a fan, or if you tried the original and couldn't get a handle on it, jump in with both feet now! Everything you really need to know about the Doctor, they'll tell you as they go along. This series was written with minimal references to the Doctor's enormous back story specifically to encourage new viewers. Admittedly, I'm only seeing the first new series now as it's being shown on the Sci-Fi channel (in other words, probably cut to ribbons for time constraints), but I'm looking forward to future episodes either on broadcast TV or on DVD. (July 4th can't come soon enough!)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentiment\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"negative\",\n          \"positive\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"review_sentiment\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"}},"metadata":{},"execution_count":35}]},{"cell_type":"code","source":["\n","from sklearn.model_selection import train_test_split\n","\n","train_X, test_X, train_Y, test_Y = train_test_split(df['review'], df['sentiment'],test_size=0.33,\n","                                                    random_state=42)\n","print(train_X)\n","print(train_Y)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oGHnKJKRTeWP","executionInfo":{"status":"ok","timestamp":1717345354593,"user_tz":-180,"elapsed":427,"user":{"displayName":"LILIAN D. WANZARE","userId":"15710368761207199104"}},"outputId":"ccc4a3fc-cf73-4bdf-ed55-ac3e2d3ba10f"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["1522    If you don't mind having your emotions toyed w...\n","835     The story has been told before. A deadly disea...\n","358     This movie features Charlie Spradling dancing ...\n","138     I just watched this movie on it's premier nigh...\n","299     I saw Brother's Shadow at the Tribeca Film Fes...\n","                              ...                        \n","4426    ***SPOILERS*** Well made and interesting film ...\n","466     I've read the other reviews and found some to ...\n","3092    An interesting comedy, taking place on a train...\n","3772    *** WARNING! SPOILERS CONTAINED HEREIN! ***<br...\n","860     This production was quite a surprise for me. I...\n","Name: review, Length: 3350, dtype: object\n","1522    negative\n","835     positive\n","358     negative\n","138     positive\n","299     positive\n","          ...   \n","4426    positive\n","466     positive\n","3092    positive\n","3772    negative\n","860     positive\n","Name: sentiment, Length: 3350, dtype: object\n"]}]},{"cell_type":"code","source":["from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix,classification_report\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import confusion_matrix,classification_report\n","\n","tfidf_vectorizer = TfidfVectorizer()\n","\n","tfidf_train_vectors = tfidf_vectorizer.fit_transform(train_X)\n","\n","tfidf_test_vectors = tfidf_vectorizer.transform(test_X)"],"metadata":{"id":"B45w4abYa0M5","executionInfo":{"status":"ok","timestamp":1717344355025,"user_tz":-180,"elapsed":2828,"user":{"displayName":"LILIAN D. WANZARE","userId":"15710368761207199104"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["classifier = RandomForestClassifier()\n","\n","classifier.fit(tfidf_train_vectors,train_Y)\n","y_pred = classifier.predict(tfidf_test_vectors)\n","print(classification_report(test_Y,y_pred))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2dqdHfiGbWJ3","executionInfo":{"status":"ok","timestamp":1717344366974,"user_tz":-180,"elapsed":7176,"user":{"displayName":"LILIAN D. WANZARE","userId":"15710368761207199104"}},"outputId":"07f02727-66ca-41d3-abbb-14379035fb0b"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","    negative       0.82      0.85      0.83       859\n","    positive       0.83      0.79      0.81       791\n","\n","    accuracy                           0.82      1650\n","   macro avg       0.82      0.82      0.82      1650\n","weighted avg       0.82      0.82      0.82      1650\n","\n"]}]},{"cell_type":"markdown","source":["# Advanced Word Vectorization Models"],"metadata":{"id":"qeInbZ0sLulH"}},{"cell_type":"markdown","metadata":{"id":"JRZ73YCqqiw9"},"source":["###Word Embeddings\n","Luckily there is a third method that is far superior, **word embeddings**. This method keeps the order of words intact as well as encodes similar words with very similar labels. It attempts to not only encode the frequency and order of words but the meaning of those words in the sentence. It encodes each word as a dense vector that represents its context in the sentence.\n","\n","There are various approaches to creating more advanced word vectorization models for extracting features from text data. Here we will discuss a couple of them that use Google’s popular word2vecalgorithm.\n","Source: https://radimrehurek.com/gensim/models/word2vec.html\n","\n","The word2vecmodel, released in 2013 by Google, is a neural\n","network–based implementation that learns distributed vector representations of words based on continuous Bag of Words and skip-gram–based architectures.\n","\n","We will use Gensim. Gensim comes with several already pre-trained models, in the Gensim-data repository:\n","\n","\n","\n"]},{"cell_type":"code","source":[],"metadata":{"id":"v9C9rj9VtLbU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import gensim\n","from gensim.models import Word2Vec\n","import nltk\n","#nltk.download('punkt') # Only run this once\n","# tokenize corpora\n","TOKENIZED_CORPUS = [nltk.word_tokenize(sentence) for sentence in df['review']]\n","\n","# build the word2vec model on our training corpus\n","print(TOKENIZED_CORPUS[0])\n","model = Word2Vec(sentences=TOKENIZED_CORPUS, vector_size=10, window=5, min_count=1, workers=4)\n","\n","#model.save(\"word2vec.model\")\n","vector = model.wv['sky']\n","print(vector)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PwvyDMKUNiTN","executionInfo":{"status":"ok","timestamp":1717344577435,"user_tz":-180,"elapsed":30488,"user":{"displayName":"LILIAN D. WANZARE","userId":"15710368761207199104"}},"outputId":"12032418-1db3-4cc5-a527-128afc9bad2d"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["['One', 'of', 'the', 'other', 'reviewers', 'has', 'mentioned', 'that', 'after', 'watching', 'just', '1', 'Oz', 'episode', 'you', \"'ll\", 'be', 'hooked', '.', 'They', 'are', 'right', ',', 'as', 'this', 'is', 'exactly', 'what', 'happened', 'with', 'me.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'The', 'first', 'thing', 'that', 'struck', 'me', 'about', 'Oz', 'was', 'its', 'brutality', 'and', 'unflinching', 'scenes', 'of', 'violence', ',', 'which', 'set', 'in', 'right', 'from', 'the', 'word', 'GO', '.', 'Trust', 'me', ',', 'this', 'is', 'not', 'a', 'show', 'for', 'the', 'faint', 'hearted', 'or', 'timid', '.', 'This', 'show', 'pulls', 'no', 'punches', 'with', 'regards', 'to', 'drugs', ',', 'sex', 'or', 'violence', '.', 'Its', 'is', 'hardcore', ',', 'in', 'the', 'classic', 'use', 'of', 'the', 'word.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'It', 'is', 'called', 'OZ', 'as', 'that', 'is', 'the', 'nickname', 'given', 'to', 'the', 'Oswald', 'Maximum', 'Security', 'State', 'Penitentary', '.', 'It', 'focuses', 'mainly', 'on', 'Emerald', 'City', ',', 'an', 'experimental', 'section', 'of', 'the', 'prison', 'where', 'all', 'the', 'cells', 'have', 'glass', 'fronts', 'and', 'face', 'inwards', ',', 'so', 'privacy', 'is', 'not', 'high', 'on', 'the', 'agenda', '.', 'Em', 'City', 'is', 'home', 'to', 'many', '..', 'Aryans', ',', 'Muslims', ',', 'gangstas', ',', 'Latinos', ',', 'Christians', ',', 'Italians', ',', 'Irish', 'and', 'more', '....', 'so', 'scuffles', ',', 'death', 'stares', ',', 'dodgy', 'dealings', 'and', 'shady', 'agreements', 'are', 'never', 'far', 'away.', '<', 'br', '/', '>', '<', 'br', '/', '>', 'I', 'would', 'say', 'the', 'main', 'appeal', 'of', 'the', 'show', 'is', 'due', 'to', 'the', 'fact', 'that', 'it', 'goes', 'where', 'other', 'shows', 'would', \"n't\", 'dare', '.', 'Forget', 'pretty', 'pictures', 'painted', 'for', 'mainstream', 'audiences', ',', 'forget', 'charm', ',', 'forget', 'romance', '...', 'OZ', 'does', \"n't\", 'mess', 'around', '.', 'The', 'first', 'episode', 'I', 'ever', 'saw', 'struck', 'me', 'as', 'so', 'nasty', 'it', 'was', 'surreal', ',', 'I', 'could', \"n't\", 'say', 'I', 'was', 'ready', 'for', 'it', ',', 'but', 'as', 'I', 'watched', 'more', ',', 'I', 'developed', 'a', 'taste', 'for', 'Oz', ',', 'and', 'got', 'accustomed', 'to', 'the', 'high', 'levels', 'of', 'graphic', 'violence', '.', 'Not', 'just', 'violence', ',', 'but', 'injustice', '(', 'crooked', 'guards', 'who', \"'ll\", 'be', 'sold', 'out', 'for', 'a', 'nickel', ',', 'inmates', 'who', \"'ll\", 'kill', 'on', 'order', 'and', 'get', 'away', 'with', 'it', ',', 'well', 'mannered', ',', 'middle', 'class', 'inmates', 'being', 'turned', 'into', 'prison', 'bitches', 'due', 'to', 'their', 'lack', 'of', 'street', 'skills', 'or', 'prison', 'experience', ')', 'Watching', 'Oz', ',', 'you', 'may', 'become', 'comfortable', 'with', 'what', 'is', 'uncomfortable', 'viewing', '....', 'thats', 'if', 'you', 'can', 'get', 'in', 'touch', 'with', 'your', 'darker', 'side', '.']\n","[ 3.06313206e-02  2.45518506e-01  2.59630028e-02  1.76662039e-02\n","  1.23357438e-02 -2.49442205e-01  6.51331991e-02  2.95329064e-01\n"," -1.76652148e-01 -1.59665719e-01  1.04075829e-02 -1.23151623e-01\n"," -2.04213746e-02 -1.72053967e-02  5.31393476e-02 -5.74113913e-02\n","  5.32998145e-02 -1.14028804e-01  6.76391497e-02 -2.55678684e-01\n","  7.78405517e-02 -5.40437922e-02  1.99520096e-01 -7.24515244e-02\n"," -1.03211440e-01  4.81572859e-02 -9.06511247e-02  8.69803969e-03\n"," -7.66898915e-02  7.22632036e-02  1.00800432e-01 -9.07769278e-02\n","  2.03085557e-01 -9.73590463e-02 -5.16643114e-02  7.39014372e-02\n","  1.05758466e-01 -9.85969752e-02 -2.02005699e-01 -1.18681870e-01\n"," -3.56267914e-02 -8.93273801e-02 -1.44951165e-01 -2.41975226e-02\n","  1.27237126e-01 -6.77613541e-02 -8.56655836e-02 -2.25087795e-02\n","  1.44252285e-01  5.81118912e-02  8.09961557e-02 -6.77740201e-02\n"," -5.03215641e-02  3.57391499e-02 -3.65225561e-02  1.36038819e-02\n"," -1.22397756e-02 -5.02765216e-02 -8.27590227e-02  1.42983714e-04\n"," -4.52822112e-02 -4.70090695e-02  2.35466287e-02  4.58563678e-02\n"," -1.46792501e-01  1.50582060e-01 -4.37943675e-02  1.13535747e-01\n"," -2.31336847e-01  3.92631441e-02 -1.32224768e-01  1.97575137e-01\n","  9.40677449e-02  7.59552792e-02  1.91773266e-01  5.45158163e-02\n"," -2.34385282e-02 -2.06124466e-02 -2.96921320e-02  3.39340977e-02\n"," -8.70458558e-02 -1.24450698e-01 -6.42162859e-02  1.63102239e-01\n"," -6.22304231e-02 -4.82045338e-02  9.92563814e-02  1.05016492e-01\n","  1.25340238e-01  1.46602303e-01  2.09543735e-01  1.22691557e-01\n","  1.67613216e-02 -1.09500038e-02  3.13933253e-01  1.17988810e-01\n","  2.66473647e-02 -5.51590361e-02  5.44348471e-02 -4.02855538e-02]\n"]}]},{"cell_type":"markdown","source":["Each word vector is of length 10 based on the size parameter specified earlier. But when we deal with sentences and text documents, they are of unequal length, and we must carry out some form of combining and aggregation operations to make sure the number of dimensions of the final feature vectors are the same, regardless of the length of the text document, number of words, and so on.\n","\n","Typically one just takes the mean of the word vectors for the words in a sentence as the vector of the sentence."],"metadata":{"id":"SbIGMPLCQyNk"}},{"cell_type":"code","source":["from sklearn.feature_extraction.text import CountVectorizer\n","# getting the word vectors and vectorizer\n","def create_vectorizer(corpus):\n","  # Instantiating the model\n","    model = Word2Vec(sentences = corpus, vector_size  =200, min_count=1,workers=3, window =15, sg = 1)\n","    # Getting the word vectors based on the corpus\n","    wrdvecs = pd.DataFrame(model.wv.vectors, index=model.wv.index_to_key)\n","    # creating a vectorizer from the vectors\n","    vecr = CountVectorizer(vocabulary=wrdvecs.index)\n","    print(wrdvecs.shape)\n","    return wrdvecs, vecr\n","wrdvecs, vecr = create_vectorizer(TOKENIZED_CORPUS)"],"metadata":{"id":"TmCZweDMX3SE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1717345012022,"user_tz":-180,"elapsed":133056,"user":{"displayName":"LILIAN D. WANZARE","userId":"15710368761207199104"}},"outputId":"b6558d0a-8918-48eb-e9e8-f1940f98aeff"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["(56881, 200)\n"]}]},{"cell_type":"code","source":["train_vectors = vecr.transform(train_X).dot(wrdvecs)\n","test_vectors = vecr.transform(test_X).dot(wrdvecs)\n","\n","print(train_vectors.shape)\n","print(train_Y.shape)"],"metadata":{"id":"o_yD5qxydQCr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1717345482063,"user_tz":-180,"elapsed":3766,"user":{"displayName":"LILIAN D. WANZARE","userId":"15710368761207199104"}},"outputId":"fd5f9779-8cd1-4a65-ef17-441e7e4ad95b"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["(3350, 200)\n","(3350,)\n"]}]},{"cell_type":"code","source":["classifier = RandomForestClassifier()\n","\n","classifier.fit(train_vectors,train_Y)\n","y_pred = classifier.predict(test_vectors)\n","print(classification_report(test_Y,y_pred))"],"metadata":{"id":"0jGHqZNTcrgb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1717345516085,"user_tz":-180,"elapsed":5480,"user":{"displayName":"LILIAN D. WANZARE","userId":"15710368761207199104"}},"outputId":"49d3bb28-1900-4aa6-ff67-6841a468f221"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","    negative       0.80      0.79      0.79       859\n","    positive       0.77      0.79      0.78       791\n","\n","    accuracy                           0.79      1650\n","   macro avg       0.79      0.79      0.79      1650\n","weighted avg       0.79      0.79      0.79      1650\n","\n"]}]},{"cell_type":"markdown","source":["Unlike the previous techniques word embeddings are learned by looking at many different training examples.\n","We have looked at an example using pretrained word embeddings.\n","\n","You can add what's called an *embedding layer* to the beggining of your model and while your model trains your embedding layer will learn the correct embeddings for words. You can also use pretrained embedding layers.\n","\n","This is the technique we will use for our examples and its implementation will be showed later on."],"metadata":{"id":"XAPjg3PhNJoW"}},{"cell_type":"markdown","metadata":{"id":"h5cjtsHP8t5Y"},"source":["\n","\n","###Recurrent Neural Networks\n","\n","In this tutorial we will introduce a new kind of neural network that is much more capable of processing sequential data such as text or characters called a **recurrent neural network** (RNN for short).\n","\n","We will learn how to use a reccurent neural network to do the following:\n","- Sentiment Analysis\n","\n","RNN's are complex and come in many different forms so in this tutorial we wil focus on how they work and the kind of problems they are best suited for.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ehig3qliuUzk"},"source":["##Recurrent Neural Networks (RNN's)\n","Now that we've learned a little bit about how we can encode text it's time to dive into recurrent neural networks. Up until this point we have been using something called **feed-forward** neural networks. This simply means that all our data is fed forwards (all at once) from left to right through the network. This was fine for the problems we considered before but won't work very well for processing text. After all, even we (humans) don't process text all at once. We read word by word from left to right and keep track of the current meaning of the sentence so we can understand the meaning of the next word. Well this is exaclty what a recurrent neural network is designed to do. When we say recurrent neural network all we really mean is a network that contains a loop. A RNN will process one word at a time while maintaining an internal memory of what it's already seen. This will allow it to treat words differently based on their order in a sentence and to slowly build an understanding of the entire input, one word at a time.\n","\n","This is why we are treating our text data as a sequence! So that we can pass one word at a time to the RNN.\n","\n","Let's have a look at what a recurrent layer might look like.\n","\n","![alt text](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png)\n","*Source: https://colah.github.io/posts/2015-08-Understanding-LSTMs/*\n","\n","Let's define what all these variables stand for before we get into the explination.\n","\n","**h<sub>t</sub>** output at time t\n","\n","**x<sub>t</sub>** input at time t\n","\n","**A** Recurrent Layer (loop)\n","\n","What this diagram is trying to illustrate is that a recurrent layer processes words or input one at a time in a combination with the output from the previous iteration. So, as we progress further in the input sequence, we build a more complex understanding of the text as a whole.\n","\n","What we've just looked at is called a **simple RNN layer**. It can be effective at processing shorter sequences of text for simple problems but has many downfalls associated with it. One of them being the fact that as text sequences get longer it gets increasingly difficult for the network to understand the text properly.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Fo3WY-e86zX2"},"source":["##LSTM\n","The layer we dicussed in depth above was called a *simpleRNN*. However, there does exist some other recurrent layers (layers that contain a loop) that work much better than a simple RNN layer. The one we will talk about here is called LSTM (Long Short-Term Memory). This layer works very similarily to the simpleRNN layer but adds a way to access inputs from any timestep in the past. Whereas in our simple RNN layer input from previous timestamps gradually disappeared as we got further through the input. With a LSTM we have a long-term memory data structure storing all the previously seen inputs as well as when we saw them. This allows for us to access any previous value we want at any point in time. This adds to the complexity of our network and allows it to discover more useful relationships between inputs and when they appear.\n","\n","For the purpose of this course we will refrain from going any further into the math or details behind how these layers work.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"CRGOx6_v4eZ_"},"source":["##Sentiment Analysis\n","And now time to see a recurrent neural network in action. For this example, we are going to do something called sentiment analysis.\n","\n","The formal definition of this term from Wikipedia is as follows:\n","\n","*the process of computationally identifying and categorizing opinions expressed in a piece of text, especially in order to determine whether the writer's attitude towards a particular topic, product, etc. is positive, negative, or neutral.*\n","\n","The example we’ll use here is classifying movie reviews as either postive, negative or neutral.\n","\n","*This guide is based on the following tensorflow tutorial: https://www.tensorflow.org/tutorials/text/text_classification_rnn*\n","\n"]},{"cell_type":"markdown","metadata":{"id":"RACGE5Ypt5u9"},"source":["###Movie Review Dataset\n","Well start by loading in the IMDB movie review dataset from keras. This dataset contains 25,000 reviews from IMDB where each one is already preprocessed and has a label as either positive or negative. Each review is encoded by integers that represents how common a word is in the entire dataset. For example, a word encoded by the integer 3 means that it is the 3rd most common word in the dataset.\n","\n","\n","\n"]},{"cell_type":"code","source":["%tensorflow_version 2.x  # this line is not required unless you are in a notebook\n","from keras.datasets import imdb\n","VOCAB_SIZE = 88584\n","(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = VOCAB_SIZE)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NyZr7pYCe-Yh","executionInfo":{"status":"ok","timestamp":1717346172471,"user_tz":-180,"elapsed":16870,"user":{"displayName":"LILIAN D. WANZARE","userId":"15710368761207199104"}},"outputId":"5c8c834d-696d-4b11-ec07-ac45ca9fd786"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n","Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n","17464789/17464789 [==============================] - 0s 0us/step\n"]}]},{"cell_type":"code","metadata":{"id":"pdsus1kyXWC8","executionInfo":{"status":"ok","timestamp":1717346177346,"user_tz":-180,"elapsed":571,"user":{"displayName":"LILIAN D. WANZARE","userId":"15710368761207199104"}}},"source":["\n","from keras.preprocessing import sequence\n","import keras\n","import tensorflow as tf\n","import os\n","import numpy as np\n","\n","MAXLEN = 250\n","BATCH_SIZE = 64\n","\n"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wh6lOpcQ9sIZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1717341146669,"user_tz":-180,"elapsed":1194,"user":{"displayName":"LILIAN D. WANZARE","userId":"15710368761207199104"}},"outputId":"6384b8d4-312b-49cf-84cd-3dbb9b19f51b"},"source":["# Lets look at one review\n","train_data[1]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[1,\n"," 194,\n"," 1153,\n"," 194,\n"," 8255,\n"," 78,\n"," 228,\n"," 5,\n"," 6,\n"," 1463,\n"," 4369,\n"," 5012,\n"," 134,\n"," 26,\n"," 4,\n"," 715,\n"," 8,\n"," 118,\n"," 1634,\n"," 14,\n"," 394,\n"," 20,\n"," 13,\n"," 119,\n"," 954,\n"," 189,\n"," 102,\n"," 5,\n"," 207,\n"," 110,\n"," 3103,\n"," 21,\n"," 14,\n"," 69,\n"," 188,\n"," 8,\n"," 30,\n"," 23,\n"," 7,\n"," 4,\n"," 249,\n"," 126,\n"," 93,\n"," 4,\n"," 114,\n"," 9,\n"," 2300,\n"," 1523,\n"," 5,\n"," 647,\n"," 4,\n"," 116,\n"," 9,\n"," 35,\n"," 8163,\n"," 4,\n"," 229,\n"," 9,\n"," 340,\n"," 1322,\n"," 4,\n"," 118,\n"," 9,\n"," 4,\n"," 130,\n"," 4901,\n"," 19,\n"," 4,\n"," 1002,\n"," 5,\n"," 89,\n"," 29,\n"," 952,\n"," 46,\n"," 37,\n"," 4,\n"," 455,\n"," 9,\n"," 45,\n"," 43,\n"," 38,\n"," 1543,\n"," 1905,\n"," 398,\n"," 4,\n"," 1649,\n"," 26,\n"," 6853,\n"," 5,\n"," 163,\n"," 11,\n"," 3215,\n"," 10156,\n"," 4,\n"," 1153,\n"," 9,\n"," 194,\n"," 775,\n"," 7,\n"," 8255,\n"," 11596,\n"," 349,\n"," 2637,\n"," 148,\n"," 605,\n"," 15358,\n"," 8003,\n"," 15,\n"," 123,\n"," 125,\n"," 68,\n"," 23141,\n"," 6853,\n"," 15,\n"," 349,\n"," 165,\n"," 4362,\n"," 98,\n"," 5,\n"," 4,\n"," 228,\n"," 9,\n"," 43,\n"," 36893,\n"," 1157,\n"," 15,\n"," 299,\n"," 120,\n"," 5,\n"," 120,\n"," 174,\n"," 11,\n"," 220,\n"," 175,\n"," 136,\n"," 50,\n"," 9,\n"," 4373,\n"," 228,\n"," 8255,\n"," 5,\n"," 25249,\n"," 656,\n"," 245,\n"," 2350,\n"," 5,\n"," 4,\n"," 9837,\n"," 131,\n"," 152,\n"," 491,\n"," 18,\n"," 46151,\n"," 32,\n"," 7464,\n"," 1212,\n"," 14,\n"," 9,\n"," 6,\n"," 371,\n"," 78,\n"," 22,\n"," 625,\n"," 64,\n"," 1382,\n"," 9,\n"," 8,\n"," 168,\n"," 145,\n"," 23,\n"," 4,\n"," 1690,\n"," 15,\n"," 16,\n"," 4,\n"," 1355,\n"," 5,\n"," 28,\n"," 6,\n"," 52,\n"," 154,\n"," 462,\n"," 33,\n"," 89,\n"," 78,\n"," 285,\n"," 16,\n"," 145,\n"," 95]"]},"metadata":{},"execution_count":25}]},{"cell_type":"markdown","metadata":{"id":"EAtZHE9-eQ07"},"source":["###More Preprocessing\n","If we have a look at some of our loaded in reviews, we'll notice that they are different lengths. This is an issue. We cannot pass different length data into our neural network. Therefore, we must make each review the same length. To do this we will follow the procedure below:\n","- if the review is greater than 250 words then trim off the extra words\n","- if the review is less than 250 words add the necessary amount of 0's to make it equal to 250.\n","\n","Luckily for us keras has a function that can do this for us:\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"Z3qQ83sNeog6","executionInfo":{"status":"ok","timestamp":1717346183596,"user_tz":-180,"elapsed":1493,"user":{"displayName":"LILIAN D. WANZARE","userId":"15710368761207199104"}}},"source":["train_data = sequence.pad_sequences(train_data, MAXLEN)\n","test_data = sequence.pad_sequences(test_data, MAXLEN)"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mDm_0RTVir7I"},"source":["###Creating the Model\n","Now it's time to create the model. We'll use a word embedding layer as the first layer in our model and add a LSTM layer afterwards that feeds into a dense node to get our predicted sentiment.\n","\n","32 stands for the output dimension of the vectors generated by the embedding layer. We can change this value if we'd like!"]},{"cell_type":"code","metadata":{"id":"OWGGcBIpjrMu","executionInfo":{"status":"ok","timestamp":1717346190939,"user_tz":-180,"elapsed":2497,"user":{"displayName":"LILIAN D. WANZARE","userId":"15710368761207199104"}}},"source":["model = tf.keras.Sequential([\n","    tf.keras.layers.Embedding(VOCAB_SIZE, 32),\n","    tf.keras.layers.LSTM(32),\n","    tf.keras.layers.Dense(1, activation=\"sigmoid\")\n","])"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"O8_jPL_Kkr-a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1717346194574,"user_tz":-180,"elapsed":424,"user":{"displayName":"LILIAN D. WANZARE","userId":"15710368761207199104"}},"outputId":"ed36e51b-83d1-4e1d-b725-db41394bde92"},"source":["model.summary()"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, None, 32)          2834688   \n","                                                                 \n"," lstm (LSTM)                 (None, 32)                8320      \n","                                                                 \n"," dense (Dense)               (None, 1)                 33        \n","                                                                 \n","=================================================================\n","Total params: 2843041 (10.85 MB)\n","Trainable params: 2843041 (10.85 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","metadata":{"id":"eyeQCk3LlK6V"},"source":["###Training\n","Now it's time to compile and train the model."]},{"cell_type":"code","metadata":{"id":"KKEMjaIulPBe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1717346345569,"user_tz":-180,"elapsed":146098,"user":{"displayName":"LILIAN D. WANZARE","userId":"15710368761207199104"}},"outputId":"18d4d739-6a52-4657-87a0-7edad36200f7"},"source":["model.compile(loss=\"binary_crossentropy\",optimizer=\"rmsprop\",metrics=['acc'])\n","\n","history = model.fit(train_data, train_labels, epochs=10, validation_split=0.2)"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","625/625 [==============================] - 48s 71ms/step - loss: 0.4581 - acc: 0.7766 - val_loss: 0.3128 - val_acc: 0.8740\n","Epoch 2/10\n","625/625 [==============================] - 18s 28ms/step - loss: 0.2659 - acc: 0.8964 - val_loss: 0.3268 - val_acc: 0.8704\n","Epoch 3/10\n","625/625 [==============================] - 14s 23ms/step - loss: 0.2036 - acc: 0.9251 - val_loss: 0.2833 - val_acc: 0.8882\n","Epoch 4/10\n","625/625 [==============================] - 11s 17ms/step - loss: 0.1694 - acc: 0.9398 - val_loss: 0.3077 - val_acc: 0.8844\n","Epoch 5/10\n","625/625 [==============================] - 10s 15ms/step - loss: 0.1394 - acc: 0.9517 - val_loss: 0.3242 - val_acc: 0.8822\n","Epoch 6/10\n","625/625 [==============================] - 10s 15ms/step - loss: 0.1125 - acc: 0.9628 - val_loss: 0.3275 - val_acc: 0.8822\n","Epoch 7/10\n","625/625 [==============================] - 9s 14ms/step - loss: 0.0969 - acc: 0.9688 - val_loss: 0.4371 - val_acc: 0.8788\n","Epoch 8/10\n","625/625 [==============================] - 8s 12ms/step - loss: 0.0813 - acc: 0.9740 - val_loss: 0.3591 - val_acc: 0.8754\n","Epoch 9/10\n","625/625 [==============================] - 9s 15ms/step - loss: 0.0706 - acc: 0.9783 - val_loss: 0.5000 - val_acc: 0.8636\n","Epoch 10/10\n","625/625 [==============================] - 10s 15ms/step - loss: 0.0611 - acc: 0.9811 - val_loss: 0.4350 - val_acc: 0.8790\n"]}]},{"cell_type":"markdown","metadata":{"id":"3buYlkkhoK93"},"source":["And we'll evaluate the model on our training data to see how well it performs."]},{"cell_type":"code","metadata":{"id":"KImNMWTDoJaQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1717346369222,"user_tz":-180,"elapsed":5296,"user":{"displayName":"LILIAN D. WANZARE","userId":"15710368761207199104"}},"outputId":"2c1438c5-4384-4fb2-9cca-158da2366aca"},"source":["results = model.evaluate(test_data, test_labels)\n","print(results)"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["782/782 [==============================] - 4s 6ms/step - loss: 0.4895 - acc: 0.8607\n","[0.48951247334480286, 0.8607199788093567]\n"]}]},{"cell_type":"markdown","metadata":{"id":"N1RRGcr9CFCW"},"source":["So we're scoring somewhere in the mid-high 80's. Not bad for a simple recurrent network."]},{"cell_type":"markdown","metadata":{"id":"lGrBRC4YCObV"},"source":["###Making Predictions\n","Now let’s use our network to make predictions on our own reviews.\n","\n","Since our reviews are encoded well need to convert any review that we write into that form so the network can understand it. To do that well load the encodings from the dataset and use them to encode our own data.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"Onu8leY4Cn9z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1717346392476,"user_tz":-180,"elapsed":376,"user":{"displayName":"LILIAN D. WANZARE","userId":"15710368761207199104"}},"outputId":"8c60e8a4-8c66-4178-e709-d2dbe19ff6e7"},"source":["word_index = imdb.get_word_index()\n","\n","def encode_text(text):\n","  tokens = keras.preprocessing.text.text_to_word_sequence(text)\n","  tokens = [word_index[word] if word in word_index else 0 for word in tokens]\n","  return sequence.pad_sequences([tokens], MAXLEN)[0]\n","\n","text = \"that movie was just amazing, so amazing\"\n","encoded = encode_text(text)\n","print(encoded)\n"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n","1641221/1641221 [==============================] - 0s 0us/step\n","[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n","   0   0   0   0   0   0   0   0   0  12  17  13  40 477  35 477]\n"]}]},{"cell_type":"code","metadata":{"id":"PKna3vxmFwrB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1717346406253,"user_tz":-180,"elapsed":391,"user":{"displayName":"LILIAN D. WANZARE","userId":"15710368761207199104"}},"outputId":"0fde05fe-7436-450b-ab69-b942b8cbd663"},"source":["# while were at it lets make a decode function\n","\n","reverse_word_index = {value: key for (key, value) in word_index.items()}\n","\n","def decode_integers(integers):\n","    PAD = 0\n","    text = \"\"\n","    for num in integers:\n","      if num != PAD:\n","        text += reverse_word_index[num] + \" \"\n","\n","    return text[:-1]\n","\n","print(decode_integers(encoded))"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["that movie was just amazing so amazing\n"]}]},{"cell_type":"code","metadata":{"id":"L8nyrr00HPZF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1717346418215,"user_tz":-180,"elapsed":1069,"user":{"displayName":"LILIAN D. WANZARE","userId":"15710368761207199104"}},"outputId":"29ec4115-243a-40af-e05f-b8c84abe4401"},"source":["# now time to make a prediction\n","\n","def predict(text):\n","  encoded_text = encode_text(text)\n","  pred = np.zeros((1,250))\n","  pred[0] = encoded_text\n","  result = model.predict(pred)\n","  print(result[0])\n","\n","positive_review = \"That movie was! really loved it and would great watch it again because it was amazingly great\"\n","predict(positive_review)\n","\n","negative_review = \"that movie really sucked. I hated it and wouldn't watch it again. Was one of the worst things I've ever watched\"\n","predict(negative_review)\n"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 371ms/step\n","[0.99195486]\n","1/1 [==============================] - 0s 20ms/step\n","[0.3945512]\n"]}]}]}